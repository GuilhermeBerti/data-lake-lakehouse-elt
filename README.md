# ğŸ“Š Projeto Data Lake - IoT Sensors

## ğŸ“Œ VisÃ£o Geral

Este projeto demonstra a implementaÃ§Ã£o de um pipeline de dados simples
utilizando arquitetura de Data Lake na AWS.

O objetivo Ã© ingerir um arquivo CSV com leituras de sensores IoT,
armazenar os dados no Amazon S3 e realizar consultas utilizando o
Dremio.

------------------------------------------------------------------------

## ğŸ— Arquitetura do Projeto

### ğŸ”„ Fluxo de Dados

1.  **Fonte de Dados**\
    Arquivo `iot_sensors_readings.csv` armazenado em um repositÃ³rio no
    GitHub.

2.  **IngestÃ£o de Dados**\
    O Airbyte realiza a conexÃ£o com o GitHub e envia os dados para o
    Amazon S3.

    ![Airbyte](https://raw.githubusercontent.com/GuilhermeBerti/GaleriaDataEngineer/main/airbyte.png)


3.  **Armazenamento (Data Lake)**\
    Bucket utilizado:

        datalake-guilherme-dev

    Estrutura de pastas:

        raw/
          â””â”€â”€ iot_sensors_readings/
              â””â”€â”€ iot_sensors_readings/
                  â”œâ”€â”€ 2026_02_11_1770773683516_0.csv
                  â””â”€â”€ 2026_02_11_1770773322528_0.parquet

    ![Amazon S3](https://raw.githubusercontent.com/GuilhermeBerti/GaleriaDataEngineer/main/AmazonS3.png)

4.  **Consulta dos Dados**\
    O Dremio conecta diretamente ao S3 e consulta os arquivos no formato
    Parquet.

    ![Dremio](https://raw.githubusercontent.com/GuilhermeBerti/GaleriaDataEngineer/main/dremio.png)

------------------------------------------------------------------------

## ğŸ“‚ Camada RAW

Os dados sÃ£o armazenados na camada `raw`, mantendo o formato original da
ingestÃ£o.

CaracterÃ­sticas:

-   Arquivos versionados por timestamp
-   Armazenamento em CSV e Parquet
-   Dados sem transformaÃ§Ã£o
-   Estrutura organizada por dataset

Caminho no S3:

    s3://datalake-guilherme-dev/raw/iot_sensors_readings/iot_sensors_readings/

------------------------------------------------------------------------

## ğŸ“¦ Formato dos Arquivos

-   **CSV**: formato original ingerido.
-   **Parquet**: formato colunar otimizado para consulta analÃ­tica.

O Parquet Ã© utilizado para melhorar a performance das consultas
realizadas pelo Dremio.

------------------------------------------------------------------------

## ğŸ” Exemplo de Consulta no Dremio

``` sql
SELECT *
FROM "datalake-s3"."datalake-guilherme-dev".raw."iot_sensors_readings"."iot_sensors_readings"."2026_02_11_1770773322528_0.parquet";
```

Essa consulta permite visualizar todos os registros armazenados no Data
Lake.

## ğŸ“Š AnÃ¡lise de Leituras por Status

A consulta abaixo realiza uma agregaÃ§Ã£o dos dados armazenados no Data Lake, agrupando os registros pelo campo `status` dos sensores.

O objetivo Ã© identificar a quantidade de leituras por status, contabilizando:

- Total de `sensor_id`
- Total de registros de `timestamp`

```sql
SELECT 
    status, 
    COUNT(sensor_id) AS Count_sensor_id, 
    COUNT("2026_02_11_1770773322528_0.parquet"."timestamp") AS Count_timestamp
FROM "datalake-s3"."datalake-guilherme-dev".raw.iot_sensors_readings.iot_sensors_readings."2026_02_11_1770773322528_0.parquet" AS "2026_02_11_1770773322528_0.parquet"
GROUP BY status;
```

Essa consulta permite analisar a distribuiÃ§Ã£o dos estados dos sensores e validar a integridade dos dados ingeridos no Data Lake.

------------------------------------------------------------------------

## ğŸ›  Tecnologias Utilizadas

-   GitHub (Fonte de Dados)
-   Airbyte (IngestÃ£o)
-   AWS S3 (Armazenamento)
-   Parquet (Formato de Dados)
-   Dremio (Consulta SQL)

------------------------------------------------------------------------

## ğŸ‘¨â€ğŸ’» Autor

Guilherme Berti\
Engenheiro de Dados
